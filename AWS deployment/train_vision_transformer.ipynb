{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.25.42 requires botocore==1.27.42, but you have botocore 1.27.66 which is incompatible.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.27.66 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install sagemaker transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.108.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're using the output from a SageMaker Processing job\n",
    "input_path = 's3://sagemaker-eu-central-1-843182712965/sagemaker-scikit-learn-2022-09-03-18-17-21-834/output'\n",
    "\n",
    "train_input_path = '{}/{}'.format(input_path, 'train_data')\n",
    "valid_input_path = '{}/{}'.format(input_path, 'valid_data')\n",
    "test_input_path  = '{}/{}'.format(input_path, 'test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'epochs': 3,\n",
    "    'model_name': 'google/vit-base-patch16-224-in21k',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'train_hf_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    # Fine-tuning script\n",
    "    entry_point=entry_point,\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-03 23:36:07 Starting - Starting the training job...\n",
      "2022-09-03 23:36:32 Starting - Preparing the instances for trainingProfilerReport-1662248167: InProgress\n",
      "......\n",
      "2022-09-03 23:37:32 Downloading - Downloading input data...\n",
      "2022-09-03 23:38:02 Training - Downloading the training image..............................\n",
      "2022-09-03 23:43:04 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:07,049 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:07,074 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:07,080 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:07,550 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"model_name\": \"google/vit-base-patch16-224-in21k\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-03-23-36-07-225\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-03-23-36-07-225/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_hf_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_hf_trainer.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"model_name\":\"google/vit-base-patch16-224-in21k\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_hf_trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_hf_trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-03-23-36-07-225/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"model_name\":\"google/vit-base-patch16-224-in21k\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-pytorch-training-2022-09-03-23-36-07-225\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-03-23-36-07-225/source/sourcedir.tar.gz\",\"module_name\":\"train_hf_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_hf_trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--model_name\",\"google/vit-base-patch16-224-in21k\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=google/vit-base-patch16-224-in21k\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_hf_trainer.py --epochs 3 --model_name google/vit-base-patch16-224-in21k\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:11,587 - __main__ - INFO -  loaded train_dataset length is: 270\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:11,587 - __main__ - INFO -  loaded valid_dataset length is: 34\u001b[0m\n",
      "\u001b[34m2022-09-03 23:43:11,587 - __main__ - INFO -  loaded test_dataset length is: 34\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 3.19kB [00:00, 3.34MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/502 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 502/502 [00:00<00:00, 510kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/330M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 8.52M/330M [00:00<00:03, 89.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 17.0M/330M [00:00<00:03, 89.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 26.0M/330M [00:00<00:03, 91.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 34.7M/330M [00:00<00:03, 89.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 43.4M/330M [00:00<00:03, 89.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 52.1M/330M [00:00<00:03, 90.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 60.8M/330M [00:00<00:03, 90.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 69.7M/330M [00:00<00:02, 91.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 78.3M/330M [00:00<00:02, 90.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▋       | 87.0M/330M [00:01<00:02, 90.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 95.7M/330M [00:01<00:04, 53.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 103M/330M [00:01<00:04, 53.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 111M/330M [00:01<00:03, 60.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 120M/330M [00:01<00:03, 67.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 128M/330M [00:01<00:02, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 136M/330M [00:01<00:02, 75.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 144M/330M [00:01<00:02, 79.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 153M/330M [00:02<00:02, 82.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 161M/330M [00:02<00:02, 80.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 170M/330M [00:02<00:02, 82.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 178M/330M [00:02<00:01, 84.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 186M/330M [00:02<00:02, 54.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▊    | 193M/330M [00:02<00:02, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 201M/330M [00:02<00:02, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 210M/330M [00:03<00:01, 68.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 218M/330M [00:03<00:01, 73.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 229M/330M [00:03<00:01, 83.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 240M/330M [00:03<00:01, 91.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 250M/330M [00:03<00:00, 95.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 259M/330M [00:03<00:00, 96.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 269M/330M [00:03<00:00, 99.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 279M/330M [00:03<00:00, 101MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 289M/330M [00:03<00:00, 86.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 298M/330M [00:04<00:00, 51.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 308M/330M [00:04<00:00, 61.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 317M/330M [00:04<00:00, 69.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 327M/330M [00:04<00:00, 77.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 330M/330M [00:04<00:00, 75.8MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 270\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 270\n",
      "  Num Epochs = 3\u001b[0m\n",
      "\u001b[34mNum Epochs = 3\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\u001b[0m\n",
      "\u001b[34m0%|          | 0/81 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.713 algo-1:28 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.884 algo-1:28 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.885 algo-1:28 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.886 algo-1:28 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.886 algo-1:28 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:22.886 algo-1:28 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.661 algo-1:28 INFO hook.py:560] name:vit.embeddings.cls_token count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.embeddings.position_embeddings count_params:151296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.embeddings.patch_embeddings.projection.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.embeddings.patch_embeddings.projection.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.662 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.663 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.664 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.665 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.666 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.667 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.668 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.layernorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:vit.layernorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:560] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:562] Total Trainable Params: 85800194\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.669 algo-1:28 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-09-03 23:43:23.670 algo-1:28 INFO hook.py:485] Hook is writing from the hook with pid: 28\u001b[0m\n",
      "\u001b[34m1%|          | 1/81 [00:05<07:53,  5.92s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/81 [00:06<03:35,  2.73s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 3/81 [00:06<02:12,  1.70s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 4/81 [00:07<01:34,  1.23s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 5/81 [00:07<01:12,  1.04it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 6/81 [00:08<01:01,  1.21it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 7/81 [00:08<00:52,  1.41it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 8/81 [00:09<00:46,  1.59it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 9/81 [00:09<00:41,  1.72it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 10/81 [00:10<00:38,  1.85it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 11/81 [00:10<00:36,  1.94it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 12/81 [00:11<00:34,  2.01it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 13/81 [00:11<00:33,  2.06it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 14/81 [00:12<00:32,  2.09it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 15/81 [00:12<00:31,  2.12it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 16/81 [00:13<00:30,  2.12it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 17/81 [00:13<00:30,  2.12it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 18/81 [00:13<00:29,  2.12it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 19/81 [00:14<00:29,  2.14it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 20/81 [00:14<00:28,  2.16it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 21/81 [00:15<00:27,  2.17it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 22/81 [00:15<00:27,  2.18it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 23/81 [00:16<00:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 24/81 [00:16<00:25,  2.21it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 25/81 [00:17<00:26,  2.08it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 26/81 [00:17<00:26,  2.08it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 27/81 [00:18<00:25,  2.09it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 34\n",
      "  Batch size = 4\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 34\n",
      "  Batch size = 4\u001b[0m\n",
      "\u001b[34m0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 2/9 [00:00<00:00, 16.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 4/9 [00:00<00:00, 10.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 6/9 [00:00<00:00,  9.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 8/9 [00:00<00:00,  9.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.696314811706543, 'eval_accuracy': 0.47058823529411764, 'eval_runtime': 0.9761, 'eval_samples_per_second': 34.832, 'eval_steps_per_second': 9.22, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 27/81 [00:19<00:25,  2.09it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 9/9 [00:00<00:00,  9.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-27\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-27\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-27/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-27/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-27/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-27/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m35%|███▍      | 28/81 [00:21<01:03,  1.20s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 29/81 [00:21<00:50,  1.02it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 30/81 [00:22<00:41,  1.22it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 31/81 [00:22<00:35,  1.41it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 32/81 [00:22<00:31,  1.58it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 33/81 [00:23<00:27,  1.71it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 34/81 [00:23<00:25,  1.83it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 35/81 [00:24<00:23,  1.93it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 36/81 [00:24<00:22,  1.99it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 37/81 [00:25<00:21,  2.04it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 38/81 [00:25<00:21,  1.98it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 39/81 [00:26<00:20,  2.01it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 40/81 [00:26<00:20,  2.04it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 41/81 [00:27<00:19,  2.06it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 42/81 [00:27<00:18,  2.07it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 43/81 [00:28<00:18,  2.10it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 44/81 [00:28<00:17,  2.12it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 45/81 [00:29<00:17,  2.12it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 46/81 [00:29<00:16,  2.13it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 47/81 [00:29<00:15,  2.13it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 48/81 [00:30<00:15,  2.13it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 49/81 [00:30<00:15,  2.13it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 50/81 [00:31<00:14,  2.13it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 51/81 [00:31<00:14,  2.13it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 52/81 [00:32<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 53/81 [00:32<00:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 54/81 [00:33<00:12,  2.20it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 34\n",
      "  Batch size = 4\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 34\n",
      "  Batch size = 4\u001b[0m\n",
      "\u001b[34m0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 2/9 [00:00<00:00, 18.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 4/9 [00:00<00:00,  8.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 6/9 [00:00<00:00,  8.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 8/9 [00:00<00:00,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6779345870018005, 'eval_accuracy': 0.6764705882352942, 'eval_runtime': 1.0486, 'eval_samples_per_second': 32.423, 'eval_steps_per_second': 8.583, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 54/81 [00:34<00:12,  2.20it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 9/9 [00:00<00:00,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-54\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-54\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-54/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-54/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-54/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-54/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 55/81 [00:36<00:30,  1.18s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 56/81 [00:36<00:24,  1.03it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 57/81 [00:37<00:19,  1.22it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 58/81 [00:37<00:16,  1.40it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 59/81 [00:37<00:14,  1.55it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 60/81 [00:38<00:12,  1.69it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 61/81 [00:38<00:11,  1.81it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 62/81 [00:39<00:10,  1.88it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 63/81 [00:39<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 64/81 [00:40<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 65/81 [00:40<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 66/81 [00:41<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 67/81 [00:41<00:06,  2.09it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 68/81 [00:42<00:06,  1.97it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 69/81 [00:42<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 70/81 [00:43<00:05,  1.98it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 71/81 [00:43<00:05,  1.97it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 72/81 [00:44<00:04,  2.02it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 73/81 [00:44<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 74/81 [00:45<00:03,  2.07it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 75/81 [00:45<00:02,  2.11it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 76/81 [00:46<00:02,  2.13it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 77/81 [00:46<00:01,  2.13it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 78/81 [00:47<00:01,  2.12it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 79/81 [00:47<00:00,  2.13it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 80/81 [00:48<00:00,  2.12it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 81/81 [00:48<00:00,  2.11it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 34\u001b[0m\n",
      "\u001b[34mBatch size = 4\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 34\n",
      "  Batch size = 4\u001b[0m\n",
      "\u001b[34m0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 2/9 [00:00<00:00, 17.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 4/9 [00:00<00:00, 11.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 6/9 [00:00<00:00, 10.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 8/9 [00:00<00:00,  8.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6424574851989746, 'eval_accuracy': 0.6764705882352942, 'eval_runtime': 1.0285, 'eval_samples_per_second': 33.058, 'eval_steps_per_second': 8.751, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 81/81 [00:49<00:00,  2.11it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 9/9 [00:00<00:00,  8.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-81\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-81\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-81/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-81/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-81/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-81/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-54 (score: 0.6764705882352942).\u001b[0m\n",
      "\u001b[34mLoading best model from /opt/ml/model/checkpoint-54 (score: 0.6764705882352942).\u001b[0m\n",
      "\u001b[34m{'train_runtime': 51.1954, 'train_samples_per_second': 15.822, 'train_steps_per_second': 1.582, 'train_loss': 0.6592781161084588, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 81/81 [00:51<00:00,  2.11it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 81/81 [00:51<00:00,  1.58it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `ViTForImageClassification.forward` and have been ignored: img. If img are not expected by `ViTForImageClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 34\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 34\u001b[0m\n",
      "\u001b[34mBatch size = 4\u001b[0m\n",
      "\u001b[34mBatch size = 4\u001b[0m\n",
      "\u001b[34m0%|          | 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 3/9 [00:00<00:00, 16.85it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 5/9 [00:00<00:00, 13.03it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 7/9 [00:00<00:00, 12.00it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9/9 [00:00<00:00, 12.88it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9/9 [00:00<00:00, 12.97it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2022-09-03 23:44:15,940 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-03 23:44:15,940 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-03 23:44:15,940 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-09-03 23:44:34 Uploading - Uploading generated training model\n",
      "2022-09-03 23:51:16 Completed - Training job completed\n",
      "ProfilerReport-1662248167: NoIssuesFound\n",
      "Training seconds: 804\n",
      "Billable seconds: 804\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "    {'train': train_input_path, \n",
    "     'valid': valid_input_path,\n",
    "     'test': test_input_path,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-03-23-36-07-225/output/model.tar.gz'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $huggingface_estimator.model_data\n",
    "aws s3 cp $1 model-hf.tar.gz\n",
    "tar tvfz model-hf.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace container doesn't support deployment for image classification tasks so we have to use pytorch to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'train_pytorch_lightning.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    # Fine-tuning script\n",
    "    entry_point=entry_point,\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version='4.17.0',  # Need >= 4.10 because of https://github.com/huggingface/transformers/issues/12904\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 18:02:18 Starting - Starting the training job...\n",
      "2022-09-10 18:02:42 Starting - Preparing the instances for trainingProfilerReport-1662832938: InProgress\n",
      "......\n",
      "2022-09-10 18:03:42 Downloading - Downloading input data...\n",
      "2022-09-10 18:04:17 Training - Downloading the training image.................................\n",
      "2022-09-10 18:09:44 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-10 18:09:38,270 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-10 18:09:38,296 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-10 18:09:38,306 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-10 18:09:39,265 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"valid\": \"/opt/ml/input/data/valid\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"model_name\": \"google/vit-base-patch16-224-in21k\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"valid\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-10-18-02-18-006\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-10-18-02-18-006/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_pytorch_lightning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_pytorch_lightning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"model_name\":\"google/vit-base-patch16-224-in21k\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_pytorch_lightning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"valid\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_pytorch_lightning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-10-18-02-18-006/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"valid\":\"/opt/ml/input/data/valid\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"model_name\":\"google/vit-base-patch16-224-in21k\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"valid\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-pytorch-training-2022-09-10-18-02-18-006\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-10-18-02-18-006/source/sourcedir.tar.gz\",\"module_name\":\"train_pytorch_lightning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_pytorch_lightning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--model_name\",\"google/vit-base-patch16-224-in21k\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALID=/opt/ml/input/data/valid\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=google/vit-base-patch16-224-in21k\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_pytorch_lightning.py --epochs 3 --model_name google/vit-base-patch16-224-in21k\u001b[0m\n",
      "\u001b[34mCollecting pytorch_lightning\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.7.5-py3-none-any.whl (706 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 706.6/706.6 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=2.9.1\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 94.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (4.63.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.9.* in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (1.10.2+cu113)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.6/419.6 kB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (63.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 84.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 9.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.48.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 91.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 kB 36.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 34.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 10.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, tensorboard-data-server, pyDeprecate, pyasn1-modules, oauthlib, grpcio, cachetools, absl-py, torchmetrics, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, pytorch_lightning\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.2.0 cachetools-5.2.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.48.1 markdown-3.4.1 oauthlib-3.2.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pytorch_lightning-1.7.5 requests-oauthlib-1.3.1 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.9.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\u001b[0m\n",
      "\u001b[34mReceived args:  Namespace(epochs=3, eval_batch_size=4, model_dir='/opt/ml/model', model_name='google/vit-base-patch16-224-in21k', n_gpus='1', test_dir='/opt/ml/input/data/test', train_batch_size=10, train_dir='/opt/ml/input/data/train', valid_dir='/opt/ml/input/data/valid')\u001b[0m\n",
      "\u001b[34mLoading data sets...\u001b[0m\n",
      "\u001b[34mDataset({\n",
      "    features: ['label', 'img', 'pixel_values'],\n",
      "    num_rows: 270\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mDataset({\n",
      "    features: ['label', 'img', 'pixel_values'],\n",
      "    num_rows: 34\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mDataset({\n",
      "    features: ['label', 'img', 'pixel_values'],\n",
      "    num_rows: 34\u001b[0m\n",
      "\u001b[34m})\u001b[0m\n",
      "\u001b[34mBuilding data loaders...\u001b[0m\n",
      "\u001b[34mTraining model...\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/502 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 502/502 [00:00<00:00, 510kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/330M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 5.93M/330M [00:00<00:05, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 11.9M/330M [00:00<00:05, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 20.6M/330M [00:00<00:04, 74.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 29.7M/330M [00:00<00:03, 83.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 39.0M/330M [00:00<00:03, 88.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 48.0M/330M [00:00<00:03, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 56.8M/330M [00:00<00:03, 90.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 65.7M/330M [00:00<00:03, 91.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 74.5M/330M [00:00<00:03, 89.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 83.0M/330M [00:01<00:02, 88.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 91.9M/330M [00:01<00:02, 89.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 101M/330M [00:01<00:02, 92.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 111M/330M [00:01<00:02, 94.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 120M/330M [00:01<00:02, 94.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 129M/330M [00:01<00:02, 94.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 138M/330M [00:01<00:02, 95.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 147M/330M [00:01<00:02, 95.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 157M/330M [00:01<00:01, 96.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 166M/330M [00:01<00:01, 96.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 176M/330M [00:02<00:01, 97.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 185M/330M [00:02<00:01, 91.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▊    | 194M/330M [00:02<00:01, 91.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████▏   | 203M/330M [00:02<00:01, 91.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 212M/330M [00:02<00:01, 93.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 221M/330M [00:02<00:01, 93.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 230M/330M [00:02<00:01, 92.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 239M/330M [00:02<00:01, 91.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 247M/330M [00:02<00:00, 91.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 257M/330M [00:02<00:00, 93.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 266M/330M [00:03<00:00, 93.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 275M/330M [00:03<00:00, 94.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 284M/330M [00:03<00:00, 95.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 294M/330M [00:03<00:00, 96.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 303M/330M [00:03<00:00, 96.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 312M/330M [00:03<00:00, 96.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 321M/330M [00:03<00:00, 95.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 330M/330M [00:03<00:00, 92.2MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus='1')` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices='1')` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mMissing logger folder: /opt/ml/code/lightning_logs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m| Name       | Type     | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------\u001b[0m\n",
      "\u001b[34m0 | vit        | ViTModel | 86.4 M\u001b[0m\n",
      "\u001b[34m1 | dropout    | Dropout  | 0     \u001b[0m\n",
      "\u001b[34m2 | classifier | Linear   | 1.5 K \u001b[0m\n",
      "\u001b[34m----------------------------------------\u001b[0m\n",
      "\u001b[34m86.4 M    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m86.4 M    Total params\u001b[0m\n",
      "\u001b[34m345.563   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.660 algo-1:28 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220724-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.824 algo-1:28 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.825 algo-1:28 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.825 algo-1:28 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.826 algo-1:28 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:04.826 algo-1:28 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.670 algo-1:28 INFO hook.py:560] name:vit.embeddings.cls_token count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.embeddings.position_embeddings count_params:151296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.embeddings.patch_embeddings.projection.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.embeddings.patch_embeddings.projection.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.0.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.671 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.1.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.672 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.2.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.3.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.673 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.4.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.5.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.674 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.6.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.7.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.675 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.8.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.9.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.676 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.10.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.attention.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_before.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_before.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_after.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.677 algo-1:28 INFO hook.py:560] name:vit.encoder.layer.11.layernorm_after.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:vit.layernorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:vit.layernorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:vit.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:vit.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:560] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:562] Total Trainable Params: 86390786\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.678 algo-1:28 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-09-10 18:10:05.679 algo-1:28 INFO hook.py:485] Hook is writing from the hook with pid: 28\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/module.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=self.device)\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:  50%|█████     | 1/2 [00:05<00:05,  5.50s/it]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (27) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   3%|▎         | 1/36 [00:01<00:36,  1.05s/it]\u001b[0m\n",
      "\u001b[34mEpoch 0:   3%|▎         | 1/36 [00:01<00:36,  1.05s/it, loss=0.739, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   6%|▌         | 2/36 [00:01<00:23,  1.47it/s, loss=0.739, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   6%|▌         | 2/36 [00:01<00:23,  1.45it/s, loss=0.711, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 3/36 [00:01<00:18,  1.79it/s, loss=0.711, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   8%|▊         | 3/36 [00:01<00:18,  1.77it/s, loss=0.701, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  11%|█         | 4/36 [00:02<00:16,  1.99it/s, loss=0.701, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  11%|█         | 4/36 [00:02<00:16,  1.97it/s, loss=0.691, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  14%|█▍        | 5/36 [00:02<00:14,  2.15it/s, loss=0.691, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  14%|█▍        | 5/36 [00:02<00:14,  2.14it/s, loss=0.68, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 6/36 [00:02<00:13,  2.27it/s, loss=0.68, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 6/36 [00:02<00:13,  2.26it/s, loss=0.667, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  19%|█▉        | 7/36 [00:02<00:12,  2.36it/s, loss=0.667, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  19%|█▉        | 7/36 [00:02<00:12,  2.35it/s, loss=0.663, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 8/36 [00:03<00:11,  2.44it/s, loss=0.663, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 8/36 [00:03<00:11,  2.42it/s, loss=0.664, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▌       | 9/36 [00:03<00:10,  2.50it/s, loss=0.664, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  25%|██▌       | 9/36 [00:03<00:10,  2.48it/s, loss=0.664, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  28%|██▊       | 10/36 [00:03<00:10,  2.55it/s, loss=0.664, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  28%|██▊       | 10/36 [00:03<00:10,  2.54it/s, loss=0.662, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  31%|███       | 11/36 [00:04<00:09,  2.59it/s, loss=0.662, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  31%|███       | 11/36 [00:04<00:09,  2.58it/s, loss=0.654, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 12/36 [00:04<00:09,  2.63it/s, loss=0.654, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  33%|███▎      | 12/36 [00:04<00:09,  2.62it/s, loss=0.649, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  36%|███▌      | 13/36 [00:04<00:08,  2.66it/s, loss=0.649, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  36%|███▌      | 13/36 [00:04<00:08,  2.65it/s, loss=0.641, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 14/36 [00:05<00:08,  2.69it/s, loss=0.641, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 14/36 [00:05<00:08,  2.68it/s, loss=0.632, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  42%|████▏     | 15/36 [00:05<00:07,  2.71it/s, loss=0.632, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  42%|████▏     | 15/36 [00:05<00:07,  2.70it/s, loss=0.626, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 16/36 [00:05<00:07,  2.73it/s, loss=0.626, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 16/36 [00:05<00:07,  2.73it/s, loss=0.617, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  47%|████▋     | 17/36 [00:06<00:06,  2.75it/s, loss=0.617, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  47%|████▋     | 17/36 [00:06<00:06,  2.75it/s, loss=0.61, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|█████     | 18/36 [00:06<00:06,  2.77it/s, loss=0.61, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  50%|█████     | 18/36 [00:06<00:06,  2.76it/s, loss=0.606, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  53%|█████▎    | 19/36 [00:06<00:06,  2.79it/s, loss=0.606, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  53%|█████▎    | 19/36 [00:06<00:06,  2.78it/s, loss=0.605, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 20/36 [00:07<00:05,  2.80it/s, loss=0.605, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 20/36 [00:07<00:05,  2.79it/s, loss=0.602, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 21/36 [00:07<00:05,  2.81it/s, loss=0.602, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  58%|█████▊    | 21/36 [00:07<00:05,  2.81it/s, loss=0.587, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 22/36 [00:07<00:04,  2.83it/s, loss=0.587, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 22/36 [00:07<00:04,  2.82it/s, loss=0.571, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  64%|██████▍   | 23/36 [00:08<00:04,  2.84it/s, loss=0.571, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  64%|██████▍   | 23/36 [00:08<00:04,  2.83it/s, loss=0.555, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|██████▋   | 24/36 [00:08<00:04,  2.85it/s, loss=0.555, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  67%|██████▋   | 24/36 [00:08<00:04,  2.84it/s, loss=0.543, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  69%|██████▉   | 25/36 [00:08<00:03,  2.86it/s, loss=0.543, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  69%|██████▉   | 25/36 [00:08<00:03,  2.85it/s, loss=0.536, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  72%|███████▏  | 26/36 [00:09<00:03,  2.87it/s, loss=0.536, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  72%|███████▏  | 26/36 [00:09<00:03,  2.86it/s, loss=0.532, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  75%|███████▌  | 27/36 [00:09<00:03,  2.87it/s, loss=0.532, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  75%|███████▌  | 27/36 [00:09<00:03,  2.87it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  11%|█         | 1/9 [00:00<00:00, 26.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 28/36 [00:09<00:02,  2.86it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  22%|██▏       | 2/9 [00:00<00:00, 23.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  81%|████████  | 29/36 [00:09<00:02,  2.95it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  33%|███▎      | 3/9 [00:00<00:00, 23.07it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 30/36 [00:09<00:01,  3.04it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  44%|████▍     | 4/9 [00:00<00:00, 22.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  86%|████████▌ | 31/36 [00:09<00:01,  3.13it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  56%|█████▌    | 5/9 [00:00<00:00, 22.54it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  89%|████████▉ | 32/36 [00:09<00:01,  3.21it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  67%|██████▋   | 6/9 [00:00<00:00, 22.29it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  92%|█████████▏| 33/36 [00:10<00:00,  3.30it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  78%|███████▊  | 7/9 [00:00<00:00, 22.23it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  94%|█████████▍| 34/36 [00:10<00:00,  3.38it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  89%|████████▉ | 8/9 [00:00<00:00, 22.24it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0:  97%|█████████▋| 35/36 [00:10<00:00,  3.46it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 9/9 [00:00<00:00, 23.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 36/36 [00:10<00:00,  3.55it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 36/36 [00:10<00:00,  3.55it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 36/36 [00:10<00:00,  3.55it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/36 [00:00<?, ?it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/36 [00:00<?, ?it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   3%|▎         | 1/36 [00:00<00:24,  1.45it/s, loss=0.518, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   3%|▎         | 1/36 [00:00<00:24,  1.42it/s, loss=0.505, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   6%|▌         | 2/36 [00:01<00:17,  1.99it/s, loss=0.505, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   6%|▌         | 2/36 [00:01<00:17,  1.96it/s, loss=0.485, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 3/36 [00:01<00:14,  2.24it/s, loss=0.485, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   8%|▊         | 3/36 [00:01<00:14,  2.21it/s, loss=0.466, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  11%|█         | 4/36 [00:01<00:13,  2.41it/s, loss=0.466, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  11%|█         | 4/36 [00:01<00:13,  2.38it/s, loss=0.452, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  14%|█▍        | 5/36 [00:01<00:12,  2.52it/s, loss=0.452, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  14%|█▍        | 5/36 [00:01<00:12,  2.50it/s, loss=0.437, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 6/36 [00:02<00:11,  2.60it/s, loss=0.437, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 6/36 [00:02<00:11,  2.58it/s, loss=0.426, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  19%|█▉        | 7/36 [00:02<00:10,  2.65it/s, loss=0.426, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  19%|█▉        | 7/36 [00:02<00:11,  2.64it/s, loss=0.411, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 8/36 [00:02<00:10,  2.69it/s, loss=0.411, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 8/36 [00:02<00:10,  2.68it/s, loss=0.397, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▌       | 9/36 [00:03<00:09,  2.74it/s, loss=0.397, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  25%|██▌       | 9/36 [00:03<00:09,  2.72it/s, loss=0.389, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  28%|██▊       | 10/36 [00:03<00:09,  2.77it/s, loss=0.389, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  28%|██▊       | 10/36 [00:03<00:09,  2.76it/s, loss=0.378, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  31%|███       | 11/36 [00:03<00:08,  2.80it/s, loss=0.378, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  31%|███       | 11/36 [00:03<00:08,  2.78it/s, loss=0.359, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 12/36 [00:04<00:08,  2.82it/s, loss=0.359, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  33%|███▎      | 12/36 [00:04<00:08,  2.81it/s, loss=0.339, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  36%|███▌      | 13/36 [00:04<00:08,  2.84it/s, loss=0.339, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  36%|███▌      | 13/36 [00:04<00:08,  2.83it/s, loss=0.322, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 14/36 [00:04<00:07,  2.86it/s, loss=0.322, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 14/36 [00:04<00:07,  2.85it/s, loss=0.308, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  42%|████▏     | 15/36 [00:05<00:07,  2.87it/s, loss=0.308, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  42%|████▏     | 15/36 [00:05<00:07,  2.86it/s, loss=0.301, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 16/36 [00:05<00:06,  2.88it/s, loss=0.301, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 16/36 [00:05<00:06,  2.87it/s, loss=0.286, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  47%|████▋     | 17/36 [00:05<00:06,  2.89it/s, loss=0.286, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  47%|████▋     | 17/36 [00:05<00:06,  2.88it/s, loss=0.271, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|█████     | 18/36 [00:06<00:06,  2.90it/s, loss=0.271, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  50%|█████     | 18/36 [00:06<00:06,  2.89it/s, loss=0.256, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  53%|█████▎    | 19/36 [00:06<00:05,  2.91it/s, loss=0.256, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  53%|█████▎    | 19/36 [00:06<00:05,  2.90it/s, loss=0.242, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 20/36 [00:06<00:05,  2.92it/s, loss=0.242, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 20/36 [00:06<00:05,  2.91it/s, loss=0.229, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 21/36 [00:07<00:05,  2.92it/s, loss=0.229, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  58%|█████▊    | 21/36 [00:07<00:05,  2.92it/s, loss=0.216, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 22/36 [00:07<00:04,  2.93it/s, loss=0.216, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 22/36 [00:07<00:04,  2.92it/s, loss=0.207, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  64%|██████▍   | 23/36 [00:07<00:04,  2.94it/s, loss=0.207, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  64%|██████▍   | 23/36 [00:07<00:04,  2.93it/s, loss=0.198, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  67%|██████▋   | 24/36 [00:08<00:04,  2.94it/s, loss=0.198, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  67%|██████▋   | 24/36 [00:08<00:04,  2.93it/s, loss=0.188, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  69%|██████▉   | 25/36 [00:08<00:03,  2.95it/s, loss=0.188, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  69%|██████▉   | 25/36 [00:08<00:03,  2.94it/s, loss=0.18, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  72%|███████▏  | 26/36 [00:08<00:03,  2.95it/s, loss=0.18, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  72%|███████▏  | 26/36 [00:08<00:03,  2.94it/s, loss=0.168, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  75%|███████▌  | 27/36 [00:09<00:03,  2.95it/s, loss=0.168, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  75%|███████▌  | 27/36 [00:09<00:03,  2.95it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  11%|█         | 1/9 [00:00<00:00, 26.82it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 28/36 [00:09<00:02,  2.92it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  22%|██▏       | 2/9 [00:00<00:00, 24.03it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  81%|████████  | 29/36 [00:09<00:02,  3.01it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  33%|███▎      | 3/9 [00:00<00:00, 22.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 30/36 [00:09<00:01,  3.10it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  44%|████▍     | 4/9 [00:00<00:00, 22.05it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  86%|████████▌ | 31/36 [00:09<00:01,  3.19it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  56%|█████▌    | 5/9 [00:00<00:00, 21.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  89%|████████▉ | 32/36 [00:09<00:01,  3.28it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  67%|██████▋   | 6/9 [00:00<00:00, 22.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  92%|█████████▏| 33/36 [00:09<00:00,  3.36it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  78%|███████▊  | 7/9 [00:00<00:00, 21.96it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  94%|█████████▍| 34/36 [00:09<00:00,  3.45it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  89%|████████▉ | 8/9 [00:00<00:00, 21.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1:  97%|█████████▋| 35/36 [00:09<00:00,  3.53it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 9/9 [00:00<00:00, 22.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 36/36 [00:09<00:00,  3.62it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 36/36 [00:09<00:00,  3.62it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 36/36 [00:09<00:00,  3.62it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/36 [00:00<?, ?it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   0%|          | 0/36 [00:00<?, ?it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   3%|▎         | 1/36 [00:00<00:26,  1.34it/s, loss=0.16, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   3%|▎         | 1/36 [00:00<00:26,  1.31it/s, loss=0.152, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   6%|▌         | 2/36 [00:01<00:18,  1.88it/s, loss=0.152, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   6%|▌         | 2/36 [00:01<00:18,  1.84it/s, loss=0.14, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 3/36 [00:01<00:15,  2.13it/s, loss=0.14, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:   8%|▊         | 3/36 [00:01<00:15,  2.10it/s, loss=0.129, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  11%|█         | 4/36 [00:01<00:13,  2.31it/s, loss=0.129, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  11%|█         | 4/36 [00:01<00:13,  2.29it/s, loss=0.125, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  14%|█▍        | 5/36 [00:02<00:12,  2.44it/s, loss=0.125, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  14%|█▍        | 5/36 [00:02<00:12,  2.42it/s, loss=0.119, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 6/36 [00:02<00:11,  2.52it/s, loss=0.119, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  17%|█▋        | 6/36 [00:02<00:12,  2.50it/s, loss=0.112, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  19%|█▉        | 7/36 [00:02<00:11,  2.58it/s, loss=0.112, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  19%|█▉        | 7/36 [00:02<00:11,  2.57it/s, loss=0.107, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  22%|██▏       | 8/36 [00:03<00:10,  2.63it/s, loss=0.107, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  22%|██▏       | 8/36 [00:03<00:10,  2.62it/s, loss=0.0989, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▌       | 9/36 [00:03<00:10,  2.67it/s, loss=0.0989, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  25%|██▌       | 9/36 [00:03<00:10,  2.66it/s, loss=0.0991, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  28%|██▊       | 10/36 [00:03<00:09,  2.71it/s, loss=0.0991, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  28%|██▊       | 10/36 [00:03<00:09,  2.69it/s, loss=0.0977, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  31%|███       | 11/36 [00:04<00:09,  2.74it/s, loss=0.0977, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  31%|███       | 11/36 [00:04<00:09,  2.72it/s, loss=0.0893, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 12/36 [00:04<00:08,  2.76it/s, loss=0.0893, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  33%|███▎      | 12/36 [00:04<00:08,  2.75it/s, loss=0.0806, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  36%|███▌      | 13/36 [00:04<00:08,  2.78it/s, loss=0.0806, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  36%|███▌      | 13/36 [00:04<00:08,  2.77it/s, loss=0.0765, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  39%|███▉      | 14/36 [00:05<00:07,  2.80it/s, loss=0.0765, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  39%|███▉      | 14/36 [00:05<00:07,  2.79it/s, loss=0.0719, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  42%|████▏     | 15/36 [00:05<00:07,  2.82it/s, loss=0.0719, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  42%|████▏     | 15/36 [00:05<00:07,  2.81it/s, loss=0.0694, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  44%|████▍     | 16/36 [00:05<00:07,  2.83it/s, loss=0.0694, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  44%|████▍     | 16/36 [00:05<00:07,  2.82it/s, loss=0.0668, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  47%|████▋     | 17/36 [00:05<00:06,  2.84it/s, loss=0.0668, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  47%|████▋     | 17/36 [00:06<00:06,  2.83it/s, loss=0.0633, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|█████     | 18/36 [00:06<00:06,  2.85it/s, loss=0.0633, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  50%|█████     | 18/36 [00:06<00:06,  2.84it/s, loss=0.0599, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  53%|█████▎    | 19/36 [00:06<00:05,  2.86it/s, loss=0.0599, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  53%|█████▎    | 19/36 [00:06<00:05,  2.85it/s, loss=0.0568, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  56%|█████▌    | 20/36 [00:06<00:05,  2.87it/s, loss=0.0568, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  56%|█████▌    | 20/36 [00:06<00:05,  2.86it/s, loss=0.0553, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 21/36 [00:07<00:05,  2.88it/s, loss=0.0553, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  58%|█████▊    | 21/36 [00:07<00:05,  2.87it/s, loss=0.0517, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  61%|██████    | 22/36 [00:07<00:04,  2.88it/s, loss=0.0517, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  61%|██████    | 22/36 [00:07<00:04,  2.88it/s, loss=0.0501, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  64%|██████▍   | 23/36 [00:07<00:04,  2.89it/s, loss=0.0501, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  64%|██████▍   | 23/36 [00:07<00:04,  2.88it/s, loss=0.0485, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  67%|██████▋   | 24/36 [00:08<00:04,  2.90it/s, loss=0.0485, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  67%|██████▋   | 24/36 [00:08<00:04,  2.89it/s, loss=0.0455, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  69%|██████▉   | 25/36 [00:08<00:03,  2.90it/s, loss=0.0455, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  69%|██████▉   | 25/36 [00:08<00:03,  2.89it/s, loss=0.0435, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  72%|███████▏  | 26/36 [00:08<00:03,  2.91it/s, loss=0.0435, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  72%|███████▏  | 26/36 [00:08<00:03,  2.90it/s, loss=0.0425, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  75%|███████▌  | 27/36 [00:09<00:03,  2.91it/s, loss=0.0425, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2:  75%|███████▌  | 27/36 [00:09<00:03,  2.90it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  11%|█         | 1/9 [00:00<00:00, 27.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  78%|███████▊  | 28/36 [00:09<00:02,  2.87it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  22%|██▏       | 2/9 [00:00<00:00, 23.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  81%|████████  | 29/36 [00:09<00:02,  2.96it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  33%|███▎      | 3/9 [00:00<00:00, 22.91it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  83%|████████▎ | 30/36 [00:09<00:01,  3.04it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  44%|████▍     | 4/9 [00:00<00:00, 22.67it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  86%|████████▌ | 31/36 [00:09<00:01,  3.13it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  56%|█████▌    | 5/9 [00:00<00:00, 21.98it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  89%|████████▉ | 32/36 [00:09<00:01,  3.22it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  67%|██████▋   | 6/9 [00:00<00:00, 21.95it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  92%|█████████▏| 33/36 [00:09<00:00,  3.30it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  78%|███████▊  | 7/9 [00:00<00:00, 21.95it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  94%|█████████▍| 34/36 [00:10<00:00,  3.39it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:  89%|████████▉ | 8/9 [00:00<00:00, 21.71it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2:  97%|█████████▋| 35/36 [00:10<00:00,  3.47it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 9/9 [00:00<00:00, 22.56it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 36/36 [00:10<00:00,  3.56it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 36/36 [00:10<00:00,  3.56it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 36/36 [00:10<00:00,  3.55it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34m`Trainer.fit` stopped: `max_epochs=3` reached.\u001b[0m\n",
      "\u001b[34mEpoch 2: 100%|██████████| 36/36 [00:12<00:00,  2.79it/s, loss=0.0414, v_num=0]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1386: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[34mRestoring states from the checkpoint path at /opt/ml/code/lightning_logs/version_0/checkpoints/epoch=2-step=81.ckpt\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34mLoaded model weights from checkpoint at /opt/ml/code/lightning_logs/version_0/checkpoints/epoch=2-step=81.ckpt\u001b[0m\n",
      "\u001b[34mTesting: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTesting:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  11%|█         | 1/9 [00:00<00:00, 23.28it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  22%|██▏       | 2/9 [00:00<00:00, 23.33it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  33%|███▎      | 3/9 [00:00<00:00, 22.58it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  44%|████▍     | 4/9 [00:00<00:00, 22.05it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  56%|█████▌    | 5/9 [00:00<00:00, 21.96it/s]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  67%|██████▋   | 6/9 [00:40<00:20,  6.73s/it]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  78%|███████▊  | 7/9 [00:40<00:11,  5.77s/it]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0:  89%|████████▉ | 8/9 [00:40<00:05,  5.06s/it]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0: 100%|██████████| 9/9 [00:40<00:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34mTesting DataLoader 0: 100%|██████████| 9/9 [00:40<00:00,  4.50s/it]\u001b[0m\n",
      "\u001b[34m────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\u001b[0m\n",
      "\u001b[34m────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.8823529481887817\n",
      "        test_loss           0.30261310935020447\u001b[0m\n",
      "\u001b[34m────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
      "\u001b[34mSaving model...\u001b[0m\n",
      "\u001b[34m2022-09-10 18:11:37,445 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-10 18:11:37,445 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-10 18:11:37,445 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-09-10 18:11:44 Uploading - Uploading generated training model\n",
      "2022-09-10 18:13:49 Completed - Training job completed\n",
      "Training seconds: 607\n",
      "Billable seconds: 607\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "    {'train': train_input_path, \n",
    "     'valid': valid_input_path,\n",
    "     'test': test_input_path,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-10-18-02-18-006/output/model.tar.gz'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-central-1-843182712965/huggingface-pytorch-training-2022-09-04-12-16-25-136/output/model.tar.gz to ./model-pl.tar.gz\n",
      "-rw-r--r-- 0/0      1032158795 2022-09-04 12:25 vit.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $huggingface_estimator.model_data\n",
    "aws s3 cp $1 model-pl.tar.gz\n",
    "tar tvfz model-pl.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "rt_predictor = huggingface_estimator.deploy(initial_instance_count=1,\n",
    "                                      instance_type='ml.m5.large', \n",
    "                                      endpoint_name='HuggingFace-ViT',\n",
    "                                      wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process image to pass to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, subprocess, sys, ast, pickle, boto3\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import Dataset, Features, ClassLabel, Array3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('Validation/Farm/farmstrain-153_jpg.rf.ab54108710ba9006c6cfb037ec43b1b5.jpg')\n",
    "image = image.resize((224,224))\n",
    "image = np.array(image, dtype=np.uint8)\n",
    "image = np.moveaxis(image, source=-1, destination=0) # channels first for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be converted to json\n",
    "inputs = feature_extractor(image)\n",
    "pixel_values = inputs[\"pixel_values\"]\n",
    "pixel_values = np.array(pixel_values)\n",
    "pixel_values = pixel_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"(\\\"You need to define one of the following [\\u0027audio-classification\\u0027, \\u0027automatic-speech-recognition\\u0027, \\u0027feature-extraction\\u0027, \\u0027text-classification\\u0027, \\u0027token-classification\\u0027, \\u0027question-answering\\u0027, \\u0027table-question-answering\\u0027, \\u0027fill-mask\\u0027, \\u0027summarization\\u0027, \\u0027translation\\u0027, \\u0027text2text-generation\\u0027, \\u0027text-generation\\u0027, \\u0027zero-shot-classification\\u0027, \\u0027zero-shot-image-classification\\u0027, \\u0027conversational\\u0027, \\u0027image-classification\\u0027, \\u0027image-segmentation\\u0027, \\u0027object-detection\\u0027] as env \\u0027HF_TASK\\u0027.\\\", 403)\"\n}\n\". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/HuggingFace-ViT in account 843182712965 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24589/297499589.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrt_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m                 )\n\u001b[1;32m    513\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"(\\\"You need to define one of the following [\\u0027audio-classification\\u0027, \\u0027automatic-speech-recognition\\u0027, \\u0027feature-extraction\\u0027, \\u0027text-classification\\u0027, \\u0027token-classification\\u0027, \\u0027question-answering\\u0027, \\u0027table-question-answering\\u0027, \\u0027fill-mask\\u0027, \\u0027summarization\\u0027, \\u0027translation\\u0027, \\u0027text2text-generation\\u0027, \\u0027text-generation\\u0027, \\u0027zero-shot-classification\\u0027, \\u0027zero-shot-image-classification\\u0027, \\u0027conversational\\u0027, \\u0027image-classification\\u0027, \\u0027image-segmentation\\u0027, \\u0027object-detection\\u0027] as env \\u0027HF_TASK\\u0027.\\\", 403)\"\n}\n\". See https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logEventViewer:group=/aws/sagemaker/Endpoints/HuggingFace-ViT in account 843182712965 for more information."
     ]
    }
   ],
   "source": [
    "#still seem to have issues with using AWS endpoints for HuggingFace image classification. We will use Heroku instead\n",
    "rt_predictor.predict({\"inputs\": values })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete endpoint to not incure costs\n",
    "rt_predictor.delete_model()\n",
    "rt_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
